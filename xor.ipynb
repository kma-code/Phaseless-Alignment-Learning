{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd475a6-06cd-4d27-9bfc-57f095ddd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070a3620-6fb0-4d2c-a468-68fc4ff027fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE error\n",
    "def calc_error(r, target):\n",
    "    return r - target\n",
    "\n",
    "def calc_loss(r, target):\n",
    "    return 0.5 * ((r - target)**2).mean()\n",
    "\n",
    "def calc_accuracy_onehot(r, target):\n",
    "    # one hot encoding of output\n",
    "    b = np.zeros_like(r)\n",
    "    b[np.where(r == np.amax(r))] = 1\n",
    "    return np.linalg.norm(b * target)\n",
    "\n",
    "def calc_accuracy_single(r, target):\n",
    "    if r >= 0.5:\n",
    "        return target * 1.0\n",
    "    else:\n",
    "        return 1.0 - target\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def d_linear(x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0, x)\n",
    "\n",
    "def d_relu(x):\n",
    "    return np.heaviside(x, 0)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def d_logistic(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    y = tanh(x)\n",
    "    return 1 - y**2\n",
    "\n",
    "def ds_pinv(r_array, w_matrix):\n",
    "\n",
    "    covariance = np.cov(r_array.T)\n",
    "    mean = np.mean(r_array, axis=0)\n",
    "    gammasquared = covariance + np.outer(mean,mean)\n",
    "    \n",
    "    gamma = sqrtm(gammasquared)\n",
    "\n",
    "    gen_pseudo = np.dot(gamma, np.linalg.pinv(np.dot(w_matrix, gamma)))\n",
    "    if not np.all(np.isreal(gen_pseudo) == True):\n",
    "        raise ValueError('ds-pinv has not converged to real matrix')\n",
    "\n",
    "    return gen_pseudo\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return np.trace(A.T @ B) / np.linalg.norm(A) / np.linalg.norm(B)\n",
    "\n",
    "def sem(A):\n",
    "    return np.std(A) / np.sqrt(len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a104bcd-98f2-417d-a514-2689dcb5b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_net(lr, layers, activation, d_activation, W_init, inputs, steps, algo, learn_W, bias, learn_bias, acc_measure, bw_lr = None, alpha = None, pinv_recalc = 4, print_steps = 10):\n",
    "\n",
    "    number_layers = len(layers)\n",
    "    e = [[] for layer in range(number_layers)]\n",
    "    e_array = []\n",
    "    loss_array = []\n",
    "    acc_array = []\n",
    "    r = [[] for layer in range(number_layers)]\n",
    "    W_array = []\n",
    "    B_array = []\n",
    "    bias_array = []\n",
    "    r_array = []\n",
    "    bias_array.append(copy.deepcopy(bias))\n",
    "\n",
    "    W = copy.deepcopy(W_init)\n",
    "    \n",
    "    W_array.append(copy.deepcopy(W_init))\n",
    "    \n",
    "    if algo == 'fa':\n",
    "        B = []\n",
    "        for mat in W_init:\n",
    "            B.append(np.random.randn(*mat.shape))\n",
    "    \n",
    "    elif algo in ['pbp', 'gen-pbp', 'dyn-pbp']:\n",
    "        pinv_counter = 0\n",
    "        B = []\n",
    "        for mat in W_init:\n",
    "            B.append(np.linalg.pinv(mat))\n",
    "\n",
    "    for i in range(steps):\n",
    "        if print_steps > 0:\n",
    "            if i % (steps / print_steps) == 0:\n",
    "                print(\"Working on step \" , i)\n",
    "        dW = [np.zeros_like(W) for W in W_init]\n",
    "        dB = [np.zeros_like(W.T) for W in W_init]\n",
    "        dbias = [np.zeros_like(bias) for bias in bias]\n",
    "        \n",
    "        # batch learn over all samples\n",
    "        for j in range(len(inputs)):\n",
    "            r[0] = inputs[j]\n",
    "            target = targets[j]\n",
    "\n",
    "            # fw pass\n",
    "            for l in range(len(layers)-2):\n",
    "                r[l+1] = activation(W[l] @ r[l] + bias[l])\n",
    "            r[-1] = W[-1] @ r[-2] + bias[-1]\n",
    "\n",
    "            # bw pass\n",
    "            e[-1] = calc_error(r[-1], target)\n",
    "\n",
    "            for k in range(len(layers)-2, -1, -1):\n",
    "                if algo == 'bp':\n",
    "                    e[k] = np.diag(d_activation(r[k])) @ W[k].T @ e[k+1]\n",
    "                elif algo == 'fa':\n",
    "                    e[k] = np.diag(d_activation(r[k])) @ B[k].T @ e[k+1]\n",
    "                elif algo in ['pbp', 'gen-pbp', 'dyn-pbp']:\n",
    "                    e[k] = np.diag(d_activation(r[k])) @ B[k] @ e[k+1]\n",
    "\n",
    "            # calculate weight update\n",
    "            for l in range(len(layers)-1):\n",
    "                if learn_W[l]:\n",
    "                    dW[l] += np.outer(e[l+1], r[l])\n",
    "                if learn_bias[l]:\n",
    "                    dbias[l] += e[l+1]\n",
    "                if algo == 'dyn-pbp':\n",
    "                    dB[l] += np.outer(B[l] @ W[l] @ r[l] - r[l], W[l] @ r[l])\n",
    "                    \n",
    "            r_array.append(r.copy())           \n",
    "            e_array.append(e.copy())\n",
    "            W_array.append(W.copy())\n",
    "            if algo in ['pbp', 'gen-pbp', 'dyn-pbp']:\n",
    "                B_array.append(B.copy())\n",
    "            bias_array.append(copy.deepcopy(bias))\n",
    "            loss_array.append(calc_loss(r[-1], target))\n",
    "            \n",
    "            if acc_measure == \"onehot\":\n",
    "                acc_array.append(calc_accuracy_onehot(r[-1], target))\n",
    "            elif acc_measure == \"single\":\n",
    "                acc_array.append(calc_accuracy_single(r[-1][0], target))\n",
    "                    \n",
    "        # update weights\n",
    "        for l in range(len(layers)-1):\n",
    "            if learn_W[l]:\n",
    "                W[l] -= lr * dW[l]\n",
    "            if learn_bias[l]:\n",
    "                bias[l] -= lr * dbias[l]\n",
    "            if algo == 'dyn-pbp':\n",
    "                B[l] -= bw_lr * (dB[l] - alpha * B[l])\n",
    "        \n",
    "        if algo in ['pbp', 'gen-pbp']:\n",
    "            pinv_counter += 1\n",
    "            if pinv_counter == pinv_recalc:\n",
    "                for k in range(len(layers)-2, -1, -1):\n",
    "                    if algo == 'pbp':\n",
    "                        B[k] = np.linalg.pinv(W[k])\n",
    "                    elif algo == 'gen-pbp':\n",
    "                        # create a batch of r's since last recalc of backwards weights\n",
    "                        r_batch = [r_array[-pinv_recalc:][i][k] for i in range(pinv_recalc)]\n",
    "                        # catch an exeption where ds-pinv does not converge to a real matrix\n",
    "                        try:\n",
    "                            B[k] = ds_pinv(np.array(r_batch), W[k])\n",
    "                        except ValueError:\n",
    "                            print('ds-pinv has not converged to real matrix')\n",
    "                            return _, _, _, _, _, _\n",
    "                    pinv_counter = 0\n",
    "    if algo in ['pbp', 'gen-pbp', 'dyn-pbp']:\n",
    "        return r_array, e_array, W_array, B_array, loss_array, acc_array\n",
    "    else:\n",
    "        return r_array, e_array, W_array, _, loss_array, acc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0970cbc5-4a7a-4c76-816e-a4a56a46ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot options\n",
    "labels = ['bp', 'fa', 'pbp', 'gen-pbp', 'dyn-pbp']\n",
    "styles = ['g', 'b', 'r', 'y', 'black']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85ea58-4d2e-477a-861e-fcfa055e956b",
   "metadata": {},
   "source": [
    "# The setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091c199-2221-49df-998b-21d28edf94a6",
   "metadata": {},
   "source": [
    " We use single-target encoding.\n",
    " \n",
    " Also, bias is deactivated because we want to stress how FA is not able to produce sign changes in the error signal.\n",
    " \n",
    " Therefore, we need a net of hidden layer size 3, i.e. 2-3-1. (?)\n",
    " \n",
    " A note on pinv_recalc:\n",
    " Here, we sample over all 4 examples given by xor. Therefore, pinv-recalc should be set to 4 or multiples of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc240241-83c4-40c8-a6e0-c16750d952a5",
   "metadata": {},
   "source": [
    "# Comparison of activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4e58d-3578-4a1c-9a46-a3f8ab3f1a0a",
   "metadata": {},
   "source": [
    "Let's start by comparing the effect of different activation functions with backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a741da-d407-46dc-a4a7-11c2b600fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of model\n",
    "lr = 0.1\n",
    "layers = [2, 3, 1]\n",
    "\n",
    "inputs = np.array([[1,0], [0,1], [1,1], [0,0]])\n",
    "targets = np.array([1, 1, 0, 0])\n",
    "\n",
    "steps = 10000\n",
    "seeds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697e4e58-1fff-4060-a189-a0f0a78dfec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  0.75\n",
      "accuracy over last 100 samples:  0.75\n",
      "accuracy over last 100 samples:  0.25\n",
      "accuracy over last 100 samples:  0.75\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  0.75\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  0.75\n",
      "accuracy over last 100 samples:  0.5\n",
      "--------------------------------------\n",
      "activation:  <function relu at 0x7fbb49189790> , total accuracy over  10  seeds:  0.75  +-  0.07071067811865475\n",
      "accuracy over last 100 samples:  0.25\n",
      "accuracy over last 100 samples:  0.75\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  0.5\n",
      "accuracy over last 100 samples:  0.75\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  0.5\n",
      "accuracy over last 100 samples:  0.0\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  0.75\n",
      "--------------------------------------\n",
      "activation:  <function logistic at 0x7fbb491898b0> , total accuracy over  10  seeds:  0.65  +-  0.10124228365658293\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  1.0\n",
      "accuracy over last 100 samples:  1.0\n",
      "--------------------------------------\n",
      "activation:  <function tanh at 0x7fbb491899d0> , total accuracy over  10  seeds:  1.0  +-  0.0\n"
     ]
    }
   ],
   "source": [
    "for activation in [[relu, d_relu], [logistic, d_logistic], [tanh, d_tanh]]:\n",
    "    acc = []\n",
    "    for seed in range(seeds):\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        W1_init = np.random.normal(0, 1, size=(layers[1], layers[0]))\n",
    "        W2_init = np.random.normal(0, 1, size=(layers[2], layers[1]))\n",
    "        bias_init = [np.zeros(layers[1]), np.zeros(layers[2])]\n",
    "        W_init = [W1_init, W2_init]\n",
    "\n",
    "        _, _, _, _, _, acc_array_bp = calc_net(lr, layers, activation[0], activation[1], W_init, inputs, steps, 'bp', learn_W = [True]*(len(layers)-1), bias = bias_init, learn_bias = [False]*(len(layers)-1), acc_measure = 'single', print_steps=0)\n",
    "\n",
    "        acc.append(np.mean(acc_array_bp[-100:]))\n",
    "        print('accuracy over last 100 samples: ', acc[-1])\n",
    "\n",
    "    print('--------------------------------------')\n",
    "    print('activation: ', str(activation[0]),', total accuracy over ', seeds, ' seeds: ', np.mean(acc), ' +- ', np.std(acc)/np.sqrt(seeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e24a7d-d966-4316-8626-2987191b30f1",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "Tanh gives best results for this initialization and learning rate, so we pick this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32102422-9824-41b6-b9f1-2c879b93199f",
   "metadata": {},
   "source": [
    "# Comparison of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8068a9-6dc7-4f0e-85ae-56e4d7c10d6d",
   "metadata": {},
   "source": [
    "First, let's run a parameter scan for each model with 10 seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "576a036f-d2c6-4be2-815f-911e99ca59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of model\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "\n",
    "layers = [2, 3, 1]\n",
    "\n",
    "inputs = np.array([[1,0], [0,1], [1,1], [0,0]])\n",
    "targets = np.array([1, 1, 0, 0])\n",
    "\n",
    "steps = 2000\n",
    "seeds = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55cc3f20-4862-493a-86f0-48509da3fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: fa lr: 0.1 accuracy: 0.25 +- 0.13693063937629152\n"
     ]
    }
   ],
   "source": [
    "for model in ['fa']:\n",
    "    for lr in [0.1]:\n",
    "        acc = []\n",
    "        for seed in range(20):\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            W1_init = np.random.normal(0, 1, size=(layers[1], layers[0]))\n",
    "            W2_init = np.random.normal(0, 1, size=(layers[2], layers[1]))\n",
    "            bias_init = [np.zeros(layers[1]), np.zeros(layers[2])]\n",
    "            W_init = [W1_init, W2_init]\n",
    "\n",
    "            _, _, _, _, _, acc_array = calc_net(0.1, layers, tanh, d_tanh, W_init, inputs, steps, 'fa',\n",
    "                                            learn_W = [True]*(len(layers)-1), bias = bias_init, learn_bias = [False]*(len(layers)-1),\n",
    "                                            acc_measure = 'single', print_steps=0, pinv_recalc=4)\n",
    "\n",
    "            acc.append(acc_array)\n",
    "        print('model:', model, 'lr:', lr, 'accuracy:', np.mean(np.array(acc_array[-100:])), '+-', np.std(np.array(acc_array[-100:]))/np.sqrt(seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d8d682c-9ee1-446b-a21a-11dcf1f78f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: fa lr: 0.1 accuracy: 0.25 +- 0.13693063937629152\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for seed in range(20):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    W1_init = np.random.normal(0, 1, size=(layers[1], layers[0]))\n",
    "    W2_init = np.random.normal(0, 1, size=(layers[2], layers[1]))\n",
    "    bias_init = [np.zeros(layers[1]), np.zeros(layers[2])]\n",
    "    W_init = [W1_init, W2_init]\n",
    "\n",
    "    _, _, _, _, _, acc_array = calc_net(0.1, layers, tanh, d_tanh, W_init, inputs, steps, 'fa',\n",
    "                                    learn_W = [True]*(len(layers)-1), bias = bias_init, learn_bias = [False]*(len(layers)-1),\n",
    "                                    acc_measure = 'single', print_steps=0, pinv_recalc=4)\n",
    "\n",
    "    acc.append(acc_array)\n",
    "print('model:', model, 'lr:', lr, 'accuracy:', np.mean(np.array(acc_array[-100:])), '+-', np.std(np.array(acc_array[-100:]))/np.sqrt(seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b4533d2-9ce2-49c3-a479-1bb202b93069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: bp lr: 0.001 accuracy: 0.25 +- 0.09682458365518541\n",
      "model: bp lr: 0.01 accuracy: 0.75 +- 0.09682458365518541\n",
      "model: bp lr: 0.1 accuracy: 1.0 +- 0.0\n",
      "model: fa lr: 0.001 accuracy: 0.5 +- 0.11180339887498948\n",
      "model: fa lr: 0.01 accuracy: 0.75 +- 0.09682458365518541\n",
      "model: fa lr: 0.1 accuracy: 0.25 +- 0.09682458365518541\n",
      "model: pbp lr: 0.001 accuracy: 0.25 +- 0.09682458365518541\n",
      "model: pbp lr: 0.01 accuracy: 0.75 +- 0.09682458365518541\n",
      "model: pbp lr: 0.1 accuracy: 1.0 +- 0.0\n",
      "model: gen-pbp lr: 0.001 accuracy: 0.25 +- 0.09682458365518541\n",
      "model: gen-pbp lr: 0.01 accuracy: 0.75 +- 0.09682458365518541\n",
      "ds-pinv has not converged to real matrix\n",
      "ds-pinv has not converged to real matrix\n",
      "ds-pinv has not converged to real matrix\n",
      "ds-pinv has not converged to real matrix\n",
      "ds-pinv has not converged to real matrix\n",
      "ds-pinv has not converged to real matrix\n",
      "ds-pinv has not converged to real matrix\n",
      "ds-pinv has not converged to real matrix\n",
      "model: gen-pbp lr: 0.1 accuracy: 0.1251119550779376 +- 0.019778757021482452\n"
     ]
    }
   ],
   "source": [
    "for model in ['bp', 'fa', 'pbp', 'gen-pbp']:\n",
    "    for lr in lrs:\n",
    "        acc = []\n",
    "        for seed in range(seeds):\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            W1_init = np.random.normal(0, 1, size=(layers[1], layers[0]))\n",
    "            W2_init = np.random.normal(0, 1, size=(layers[2], layers[1]))\n",
    "            bias_init = [np.zeros(layers[1]), np.zeros(layers[2])]\n",
    "            W_init = [W1_init, W2_init]\n",
    "\n",
    "            _, _, _, _, _, acc_array = calc_net(lr, layers, tanh, d_tanh, W_init, inputs, steps, model,\n",
    "                                            learn_W = [True]*(len(layers)-1), bias = bias_init, learn_bias = [False]*(len(layers)-1),\n",
    "                                            acc_measure = 'single', print_steps=0, pinv_recalc=4)\n",
    "\n",
    "            acc.append(acc_array)\n",
    "        print('model:', model, 'lr:', lr, 'accuracy:', np.mean(np.array(acc_array[-100:])), '+-', np.std(np.array(acc_array[-100:]))/np.sqrt(seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf67f6ab-725b-4841-99dd-487a8a973269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: dyn-pbp lr: 0.001 bw_lr: 1e-05 accuracy: 0.5 +- 0.07071067811865475\n",
      "model: dyn-pbp lr: 0.001 bw_lr: 0.0001 accuracy: 0.5 +- 0.07071067811865475\n",
      "model: dyn-pbp lr: 0.001 bw_lr: 0.001 accuracy: 0.5 +- 0.07071067811865475\n",
      "model: dyn-pbp lr: 0.01 bw_lr: 1e-05 accuracy: 0.5 +- 0.07071067811865475\n",
      "model: dyn-pbp lr: 0.01 bw_lr: 0.0001 accuracy: 0.5 +- 0.07071067811865475\n",
      "model: dyn-pbp lr: 0.01 bw_lr: 0.001 accuracy: 0.5 +- 0.07071067811865475\n",
      "model: dyn-pbp lr: 0.1 bw_lr: 1e-05 accuracy: 1.0 +- 0.0\n",
      "model: dyn-pbp lr: 0.1 bw_lr: 0.0001 accuracy: 1.0 +- 0.0\n",
      "model: dyn-pbp lr: 0.1 bw_lr: 0.001 accuracy: 1.0 +- 0.0\n"
     ]
    }
   ],
   "source": [
    "# for model dyn-pseudo, we also sample over backwards learning rate\n",
    "\n",
    "bw_lrs = [1e-5, 1e-4, 1e-3]\n",
    "\n",
    "for lr in lrs:\n",
    "    for bw_lr in bw_lrs:\n",
    "        acc = []\n",
    "        for seed in range(seeds):\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            W1_init = np.random.normal(0, 1, size=(layers[1], layers[0]))\n",
    "            W2_init = np.random.normal(0, 1, size=(layers[2], layers[1]))\n",
    "            bias_init = [np.zeros(layers[1]), np.zeros(layers[2])]\n",
    "            W_init = [W1_init, W2_init]\n",
    "\n",
    "            _, _, _, _, _, acc_array = calc_net(lr, layers, tanh, d_tanh, W_init, inputs, steps, 'dyn-pbp',\n",
    "                                            learn_W = [True]*(len(layers)-1), bias = bias_init, learn_bias = [False]*(len(layers)-1),\n",
    "                                            acc_measure = 'single', print_steps=0, pinv_recalc=1, alpha = 0.1, bw_lr = bw_lr)\n",
    "\n",
    "            acc.append(acc_array)\n",
    "        print('model:', 'dyn-pbp', 'lr:', lr, 'bw_lr:', bw_lr, 'accuracy:', np.mean(np.array(acc_array[-100:])), '+-', np.std(np.array(acc_array[-100:]))/np.sqrt(seeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583cbe7-0a31-403d-95ba-8611be611d88",
   "metadata": {},
   "source": [
    "From this, we use the best values and rerun for more seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4587cae-f2bd-41a0-9916-6e5b0b295423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of model\n",
    "lr_bp = 0.1\n",
    "lr_fa = 0.01\n",
    "lr_pbp = 0.1\n",
    "lr_gen = 0.01\n",
    "lr_dyn = 0.1\n",
    "bw_lr = 1e-3\n",
    "\n",
    "layers = [2, 3, 1]\n",
    "\n",
    "inputs = np.array([[1,0], [0,1], [1,1], [0,0]])\n",
    "targets = np.array([1, 1, 0, 0])\n",
    "\n",
    "steps = 2000\n",
    "seeds = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2460cb-98b1-4cde-9a90-64f289b4dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "loss = []\n",
    "\n",
    "# for dyn-pseudo, we also record the backwards arrays\n",
    "B_array = []\n",
    "W_array = []\n",
    "r_array = []\n",
    "\n",
    "for seed in range(seeds):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    W1_init = np.random.normal(0, 1, size=(layers[1], layers[0]))\n",
    "    W2_init = np.random.normal(0, 1, size=(layers[2], layers[1]))\n",
    "    bias_init = [np.zeros(layers[1]), np.zeros(layers[2])]\n",
    "    W_init = [W1_init, W2_init]\n",
    "    \n",
    "    _, _, _, _, loss_array_bp, acc_array_bp = calc_net(lr_bp, layers, tanh, d_tanh, W_init, inputs, steps, 'bp', learn_W = [True]*(len(layers)-1), bias = bias_init, learn_bias = [False]*(len(layers)-1), acc_measure = 'single', print_steps=0)\n",
    "    _, _, _, _, loss_array_fa, acc_array_fa = calc_net(lr_fa, layers, tanh, d_tanh, W_init, inputs, steps, 'fa', learn_W = [True]*(len(layers)-1), bias = bias_init, learn_bias = [False]*(len(layers)-1), acc_measure = 'single', print_steps=0)\n",
    "    _, _, _, _, loss_array_pbp, acc_array_pbp = calc_net(lr_pbp, layers, tanh, d_tanh, W_init, inputs, steps, 'pbp', learn_W = [True]*(len(layers)-1), bias = bias_init, learn_bias = [False]*(len(layers)-1), acc_measure = 'single', print_steps=0, pinv_recalc=4)\n",
    "    _, _, _, _, loss_array_gen, acc_array_gen = calc_net(lr_gen, layers, tanh, d_tanh, W_init, inputs, steps, 'gen-pbp', learn_W = [True]*(len(layers)-1), bias = bias_init, learn_bias = [False]*(len(layers)-1), acc_measure = 'single', print_steps=0, pinv_recalc=4)\n",
    "    r, _, W, B, loss_array_dyn, acc_array_dyn = calc_net(lr_dyn, layers, tanh, d_tanh, W_init, inputs, steps, 'dyn-pbp', learn_W = [True]*(len(layers)-1), bias = bias_init,\n",
    "                                                      learn_bias = [False]*(len(layers)-1), acc_measure = 'single', print_steps=0, pinv_recalc=1, alpha = 0.1, bw_lr = bw_lr)\n",
    "    \n",
    "    B_array.append(B)\n",
    "    W_array.append(W)\n",
    "    r_array.append(r)\n",
    "    acc.append([acc_array_bp, acc_array_fa, acc_array_pbp, acc_array_gen, acc_array_dyn])\n",
    "    loss.append([loss_array_bp, loss_array_fa, loss_array_pbp, loss_array_gen, loss_array_dyn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e3b8b-b154-41bb-969d-c04811179a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total accuracy over', seeds, 'seeds: ')\n",
    "\n",
    "labels = ['bp', 'fa', 'pbp', 'gen-pbp', 'dyn-pbp']\n",
    "\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i], ':', np.mean(np.transpose(np.array(acc), axes=(1,0,2))[i]), ' +- ',  sem(np.mean(np.transpose(np.array(acc), axes=(1,0,2))[0], axis = 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89eb368-37d4-4d60-9cc9-920775fbca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_steps = 1\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    data = np.transpose(np.array(loss), axes = (1,0,2))[i]\n",
    "    plt.plot(np.linspace(0, len(data[0][::per_steps]), len(data[0][::per_steps])), np.mean(data, axis=0)[::per_steps], label=labels[i], c=styles[i])\n",
    "    \n",
    "plt.yscale('log')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2a161-4616-43ba-b18a-abc4dcc5e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(labels), 1, figsize=(10,10))\n",
    "\n",
    "per_steps = 1\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    data = np.transpose(np.array(acc), axes = (1,0,2))[i]\n",
    "    ax[i].plot(np.linspace(0, len(data[0][::per_steps]), len(data[0][::per_steps])), np.mean(data, axis=0)[::per_steps], label=labels[i], c=styles[i])\n",
    "    ax[i].legend(loc='lower right')\n",
    "\n",
    "plt.xlabel('Step')\n",
    "fig.text(0.06, 0.5, 'Accuracy', ha='center', va='center', rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d18ad57-7947-40e7-a098-d05f2d0d0b6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (1,1) not aligned: 2 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zb/q43ry99x69g19b1w0cyrrr700000gn/T/ipykernel_31646/4101203793.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_pinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zb/q43ry99x69g19b1w0cyrrr700000gn/T/ipykernel_31646/4101203793.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_pinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zb/q43ry99x69g19b1w0cyrrr700000gn/T/ipykernel_31646/1249847509.py\u001b[0m in \u001b[0;36mds_pinv\u001b[0;34m(r_array, w_matrix)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrtm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammasquared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mgen_pseudo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_pseudo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,) and (1,1) not aligned: 2 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "cos_array = []\n",
    "for layer in range(len(B_array[0][0])):\n",
    "    temp = []\n",
    "    for seed in range(len(B_array)):\n",
    "        temp.append([cos_sim(ds_pinv(r_array[seed][step][layer], W[seed][step][layer]), B_array[seed][step][layer]) for step in range(steps)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b0a4f-06c8-44f4-9c73-5c4393782b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
